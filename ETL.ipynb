{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a8d97e1-13ff-450d-9a7c-e1e92d12577b",
   "metadata": {},
   "source": [
    "# CSAT ETL Notebook\n",
    "\n",
    "This notebook outlines an ETL pipeline for analyzing Customer Satisfaction (CSAT) comments, from data loading and cleaning to text preprocessing, clustering, and saving results for further analysis in BigQuery & Looker Studio.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Installation and Importing of Required Libraries](#Installation-and-Importing-of-Required-Libraries)\n",
    "2. [Data Loading](#Data-Loading)\n",
    "3. [Data Cleaning](#Data-Cleaning)\n",
    "4. [Creating `conversations` table](#Creating-Conversations-Table)\n",
    "5. [Creating `user_comments` table](#Creating-User-Comments)\n",
    "6. [Text Preprocessing](#Text-Preprocessing)\n",
    "    - [Stopword Removal](#Stopword-Removal)\n",
    "    - [Noun Extraction](#Noun-Extraction)\n",
    "7. [Embedding Generation](#Embedding-Generation)\n",
    "8. [Clustering](#Clustering)\n",
    "9. [Saving Results](#Saving-Results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b265c42-084a-4055-bf14-435a98e508a1",
   "metadata": {},
   "source": [
    "<a name=\"Installation-and-Importing-of-Required-Libraries\"></a>\n",
    "## 1. Installation and importing of required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b437a44b-2526-4e1f-b80d-7531bf7f6803",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openpyxl\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install nltk\n",
    "!pip install spacy\n",
    "!pip install scikit-learn\n",
    "!pip install sentence-transformers\n",
    "!pip install tqdm\n",
    "\n",
    "# NLTK specific downloads\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# spaCy specific downloads for the German model\n",
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42deb372-4c4d-411a-9a9d-8d401b058eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b4345e-e8e1-4859-b9b6-985d8ac2ca59",
   "metadata": {},
   "source": [
    "<a name=\"Data-Loading\"></a>\n",
    "## 2. Data Loading\n",
    "This section covers loading the CSAT comments from an Excel file into a pandas DataFrame for processing. The file path is specified, and the data is loaded using `pd.read_excel`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "deae986a-f839-4ca4-b384-9433a23d8e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'csat_updated.xlsx'\n",
    "\n",
    "# Load the Excel file into a pandas DataFrame\n",
    "source_df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61492d3-bdf5-4c6d-90e0-560b1171321d",
   "metadata": {},
   "source": [
    "<a name=\"Data-Cleaning\"></a>\n",
    "## 3. Data Cleaning\n",
    "In this section, we clean the 'Subject' column by removing prefixes (e.g., \"RE:\", \"FW:\") and newlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "42759c9a-9e1b-417e-b372-5e7fd13ded08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_re_prefix(subject):\n",
    "    # Convert subject to string if it's not None/NaN\n",
    "    if pd.isnull(subject):\n",
    "        return subject\n",
    "    subject = str(subject)  # Ensure subject is treated as a string\n",
    "    \n",
    "    # Remove newlines\n",
    "    subject = subject.replace('\\n', ' ').replace('\\r', '')\n",
    "    \n",
    "    prefixes = ['RE: ', 'Re: ', 'RE:', 'Re:', 're: ', 're:', 'Re: FW: ', 'Re: FW:']\n",
    "    for prefix in prefixes:\n",
    "        if subject.startswith(prefix):\n",
    "            subject = subject[len(prefix):].strip()\n",
    "            break\n",
    "    return subject\n",
    "\n",
    "source_df['Subject'] = source_df['Subject'].apply(remove_re_prefix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb4bb1f-c1f0-43be-90ee-a3ba886722bf",
   "metadata": {},
   "source": [
    "<a name=\"Creating-Conversations-Table\"></a>\n",
    "## 4. Creating `conversations` table\n",
    "Here, the columns `Ratings` and `Rating comments` are removed since the source data will be normalised and the rating data will be moved to the table `user_comments`. The remaining column names are modified for consistency and ease of access. This includes renaming columns to lower case, removing spaces, periods, parentheses, and commas. The resulting table is called `conversations`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "63acc061-753c-448d-b0b9-3c7c14569dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'conversation_id', 'type', 'status', 'mailbox', 'thread_count',\n",
      "       'subject', 'created_at', 'last_modified', 'closed_at',\n",
      "       'first_response_time_office_hours_seconds', 'avg_response_time_seconds',\n",
      "       'avg_response_time_office_hours_seconds', 'replies_sent',\n",
      "       'handle_time_seconds', 'resolution_time_seconds',\n",
      "       'resolution_time_office_hours_seconds'],\n",
      "      dtype='object')\n",
      "conversations saved to 'conversations.csv'\n"
     ]
    }
   ],
   "source": [
    "conversations = source_df.drop(['Ratings', 'Rating comments'], axis=1)\n",
    "\n",
    "# Adjust the column names: make lowercase, replace spaces and periods with underscores, remove parentheses\n",
    "conversations.columns = (\n",
    "    conversations.columns.str.lower()\n",
    "    .str.replace('conversation #', 'conversation_id')\n",
    "    .str.replace(' ', '_')\n",
    "    .str.replace('.', '', regex=False)\n",
    "    .str.replace('(', '', regex=False)\n",
    "    .str.replace(')', '', regex=False)\n",
    "    .str.replace(',', '', regex=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5384d94-1537-488d-931f-935c34829472",
   "metadata": {},
   "source": [
    "<a name=\"Creating-User-Comments\"></a>\n",
    "## 5. Creating `user_comments` table\n",
    "A one-to-many relationship is identified in the source data, i.e a single conversation can have multiple ratings. Therefore, the user ratings are separated from the conversations and then merged into a table called `user_comments`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f554d971-2539-4bcb-b0b2-6a1ad425d13e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   conversation_id rating                                     rating_comment\n",
      "0           129789  great  Ganz herzlichen Dank, für die entgegenkommende...\n",
      "1           126849  great                                               None\n",
      "2           128766  great                                               None\n",
      "3           130018  great                                               None\n",
      "4           130018   okay                                               None\n",
      "5           130665  great                    Vielen lieben Dank für die Info\n",
      "6           130533  great                                               None\n",
      "7           130632  great       Vielen Dank für die nette Antwort. LG Monika\n",
      "8           130239  great                                               None\n",
      "9           110248  great                                               None\n"
     ]
    }
   ],
   "source": [
    "source_df['Rating comments'] = source_df['Rating comments'].apply(lambda x: 'None' if pd.isna(x) else x)\n",
    "\n",
    "expanded_data = []\n",
    "\n",
    "for index, row in source_df.iterrows():\n",
    "    conv_id = row['Conversation #']\n",
    "    ratings = row['Ratings'].split(',')\n",
    "    comments_str = row['Rating comments']\n",
    "\n",
    "    if comments_str not in [None, 'None']:\n",
    "        try:\n",
    "            comments = ast.literal_eval('[' + comments_str + ']')\n",
    "        except (ValueError, SyntaxError):\n",
    "            comments = [comments_str]\n",
    "    else:\n",
    "        comments = [None] * len(ratings)\n",
    "    \n",
    "    if len(comments) < len(ratings):\n",
    "        # Ensure the last rating of the same type receives the comment\n",
    "        adjusted_comments = [None] * (len(ratings) - len(comments)) + comments\n",
    "    else:\n",
    "        adjusted_comments = comments\n",
    "\n",
    "    for rating, comment in zip(ratings, adjusted_comments):\n",
    "        expanded_data.append({\n",
    "            'conversation_id': conv_id,\n",
    "            'rating': rating.strip(),\n",
    "            'rating_comment': comment if comment != 'None' else None\n",
    "        })\n",
    "\n",
    "user_comments = pd.DataFrame(expanded_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c46504c-3d98-461d-8b69-a898dcb36816",
   "metadata": {},
   "source": [
    "<a name=\"Text-Preprocessing\"></a>\n",
    "## 6. Text Preprocessing\n",
    "Text preprocessing steps include converting comments to lowercase, removing special characters, stopwords, and extracting nouns. This is facilitated by libraries such as `nltk` for stopwords removal and `spacy` for noun extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "00c0dbcb-c852-4e12-b7f9-742457fa6062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_rating_comment(comment):\n",
    "    if pd.isnull(comment):\n",
    "        return None  # Return None if comment is NaN or None\n",
    "    comment = str(comment)  # Ensure comment is treated as a string\n",
    "    # Remove newlines\n",
    "    comment = comment.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    # Remove \"None,\" or \",None\" patterns and extra whitespaces\n",
    "    comment = comment.replace(\"None,\", \"\").replace(\",None\", \"\")\n",
    "    # Trim leading and trailing whitespaces and remove surrounding double quotes\n",
    "    comment = comment.strip().strip('\"')\n",
    "    return comment\n",
    "\n",
    "# Apply the cleaning function to the 'rating_comment' column\n",
    "user_comments['rating_comment'] = user_comments['rating_comment'].apply(clean_rating_comment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "cd8847a0-82cf-45c0-bb40-1e2e418ac7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [ganz, herzlichen, dank, entgegenkommende, sch...\n",
      "1                                                 None\n",
      "2                                                 None\n",
      "3                                                 None\n",
      "4                                                 None\n",
      "Name: words, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Ensure NLTK stopwords are downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load German stopwords from NLTK\n",
    "german_stopwords = set(stopwords.words('german'))\n",
    "\n",
    "def clean_and_remove_stopwords(text):\n",
    "    # Check if text is None or empty\n",
    "    if not text:\n",
    "        return None\n",
    "    # Convert text to lowercase and tokenize by word characters, ignoring special characters\n",
    "    words = re.findall(r'\\w+', text.lower())\n",
    "    # Remove stopwords and numeric strings\n",
    "    filtered_words = [word for word in words if word not in german_stopwords and not word.isdigit()]\n",
    "    return filtered_words\n",
    "\n",
    "# Apply the function to 'rating_comment'\n",
    "user_comments['words'] = user_comments['rating_comment'].apply(clean_and_remove_stopwords)\n",
    "\n",
    "print(user_comments['words'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e7712c11-d580-4f48-9263-747cc0f6ed3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      rating_comment  \\\n",
      "0  Ganz herzlichen Dank, für die entgegenkommende...   \n",
      "1                                               None   \n",
      "2                                               None   \n",
      "3                                               None   \n",
      "4                                               None   \n",
      "\n",
      "                                 nouns  \n",
      "0  [dank, lösung, super, kundendienst]  \n",
      "1                                   []  \n",
      "2                                   []  \n",
      "3                                   []  \n",
      "4                                   []  \n"
     ]
    }
   ],
   "source": [
    "# Load the German language model\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "def extract_nouns(text):\n",
    "    if not text:\n",
    "        return []\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    # Extract nouns\n",
    "    nouns = [token.text.lower() for token in doc if token.pos_ == \"NOUN\"]\n",
    "    return nouns\n",
    "\n",
    "user_comments['nouns'] = user_comments['rating_comment'].apply(extract_nouns)\n",
    "\n",
    "print(user_comments[['rating_comment', 'nouns']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b39751e-9622-41a6-9fee-bbb5e926b957",
   "metadata": {},
   "source": [
    "<a name=\"Embedding-Generation\"></a>\n",
    "## 7. Embedding Generation\n",
    "Embeddings are generated for each comment using the `SentenceTransformer` library. These embeddings capture the semantic meaning of the comments and are used for clustering. The embedding model is multilingual, allowing similar sentences in different languages to be grouped together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "75d1a642-a061-44f4-8b23-31535bee6505",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 9057/9057 [00:45<00:00, 198.87it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>rating_comment</th>\n",
       "      <th>words</th>\n",
       "      <th>nouns</th>\n",
       "      <th>rating_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>129789</td>\n",
       "      <td>great</td>\n",
       "      <td>Ganz herzlichen Dank, für die entgegenkommende...</td>\n",
       "      <td>[ganz, herzlichen, dank, entgegenkommende, sch...</td>\n",
       "      <td>[dank, lösung, super, kundendienst]</td>\n",
       "      <td>[0.0013463228, -0.01692898, 0.0082012545, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>126849</td>\n",
       "      <td>great</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>128766</td>\n",
       "      <td>great</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>130018</td>\n",
       "      <td>great</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>130018</td>\n",
       "      <td>okay</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   conversation_id rating                                     rating_comment  \\\n",
       "0           129789  great  Ganz herzlichen Dank, für die entgegenkommende...   \n",
       "1           126849  great                                               None   \n",
       "2           128766  great                                               None   \n",
       "3           130018  great                                               None   \n",
       "4           130018   okay                                               None   \n",
       "\n",
       "                                               words  \\\n",
       "0  [ganz, herzlichen, dank, entgegenkommende, sch...   \n",
       "1                                               None   \n",
       "2                                               None   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "                                 nouns  \\\n",
       "0  [dank, lösung, super, kundendienst]   \n",
       "1                                   []   \n",
       "2                                   []   \n",
       "3                                   []   \n",
       "4                                   []   \n",
       "\n",
       "                                    rating_embedding  \n",
       "0  [0.0013463228, -0.01692898, 0.0082012545, -0.0...  \n",
       "1                                               None  \n",
       "2                                               None  \n",
       "3                                               None  \n",
       "4                                               None  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the embedder\n",
    "embedder = SentenceTransformer(\"distiluse-base-multilingual-cased\")\n",
    "\n",
    "# Initialize a list to hold the embeddings\n",
    "rating_embeddings = []\n",
    "\n",
    "# Wrap the loop with tqdm for a progress bar\n",
    "for comment in tqdm(user_comments['rating_comment'], desc=\"Generating Embeddings\"):\n",
    "    if comment is not None:\n",
    "        # Generate the embedding for the comment\n",
    "        embedding = embedder.encode([comment], show_progress_bar=False)\n",
    "        rating_embeddings.append(embedding[0])\n",
    "    else:\n",
    "        # Append None for comments that are None\n",
    "        rating_embeddings.append(None)\n",
    "\n",
    "# Add the embeddings as a new column in the user_comments DataFrame\n",
    "user_comments['rating_embedding'] = rating_embeddings\n",
    "\n",
    "user_comments.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db436d1d-458c-4dd5-821a-d80b2802ce60",
   "metadata": {},
   "source": [
    "<a name=\"Clustering\"></a>\n",
    "## 8. Clustering\n",
    "Using the embeddings, clustering is performed to group similar comments together. This helps identify common themes or issues within the feedback. The `KMeans` algorithm from `sklearn.cluster` is used for this purpose. Here, 10 clusters are created for each of the three rating categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cbc662b4-0864-4278-9a35-a5b246860b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty DataFrame for csat_impactors\n",
    "csat_impactors = pd.DataFrame(columns=['main_sentence', 'conversation_id', 'rating', 'cluster_id', 'type', 'id'])\n",
    "csat_impactors_list = []\n",
    "\n",
    "for rating in ['bad', 'okay', 'great']:\n",
    "    # Filter for current rating and non-None embeddings\n",
    "    filtered_df = user_comments[(user_comments['rating'] == rating) & (user_comments['rating_embedding'].notna())].copy()  # Ensure to work on a copy\n",
    "    \n",
    "    if filtered_df.empty:\n",
    "        continue  # Skip if no comments for this rating\n",
    "    \n",
    "    embeddings = np.array(list(filtered_df['rating_embedding']))\n",
    "    \n",
    "    # Perform clustering\n",
    "    num_clusters = 10  # Adjust as necessary\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    kmeans.fit(embeddings)\n",
    "    \n",
    "    # Assign cluster labels\n",
    "    filtered_df['cluster_id'] = ['{}_{}'.format(rating, label+1) for label in kmeans.labels_]\n",
    "    \n",
    "    # Append updated rows back to user_comments\n",
    "    for index, row in filtered_df.iterrows():\n",
    "        user_comments.at[index, 'cluster_id'] = row['cluster_id']\n",
    "        user_comments.at[index, 'local_cluster_id'] = row['cluster_id'].split(\"_\")[1]\n",
    "    \n",
    "    # Process for csat_impactors\n",
    "    for cluster_id in set(filtered_df['cluster_id']):\n",
    "        cluster_df = filtered_df[filtered_df['cluster_id'] == cluster_id]\n",
    "        cluster_embeddings = np.array(list(cluster_df['rating_embedding']))\n",
    "        cluster_center = kmeans.cluster_centers_[int(cluster_id.split('_')[-1]) - 1]\n",
    "        \n",
    "        distances = np.linalg.norm(cluster_embeddings - cluster_center, axis=1)\n",
    "        main_index = distances.argmin()\n",
    "        main_sentence = cluster_df.iloc[main_index]['rating_comment']\n",
    "        \n",
    "        csat_impactors_list.append({\n",
    "            'main_sentence': main_sentence,\n",
    "            'conversation_id': cluster_df.iloc[main_index]['conversation_id'],\n",
    "            'rating': rating,\n",
    "            'cluster_id': cluster_id.split(\"_\")[1],\n",
    "            'type': 'rating_comment',\n",
    "            'id': cluster_id\n",
    "        })\n",
    "\n",
    "csat_impactors = pd.DataFrame(csat_impactors_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2088797a-6d33-4b7a-b5cc-033adfd069fb",
   "metadata": {},
   "source": [
    "<a name=\"Saving-Results\"></a>\n",
    "## 9. Saving Results\n",
    "The final step involves saving the processed data and clustering results. `user_comments` and `csat_impactors` DataFrames are saved in both CSV and newline delimited JSON formats for further analysis with BigQuery and Looker Studio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b936d08-5d9d-427a-8d09-c33865d9f1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save conversations DataFrame to a CSV file without the index\n",
    "conversations.to_csv('conversations.csv', index=False)\n",
    "\n",
    "print(\"conversations saved to 'conversations.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8126873f-63ae-4120-abf9-9fd16fa574bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csat_impactors saved to 'csat_impactors.json'\n"
     ]
    }
   ],
   "source": [
    "# Save csat_impactors DataFrame to a newline delimited JSON file\n",
    "csat_impactors.to_json('csat_impactors.json', orient='records', lines=True)\n",
    "\n",
    "print(\"csat_impactors saved to 'csat_impactors.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8b2caa49-120f-47e7-bda1-1937b5206880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_comments saved to 'user_comments.json'\n"
     ]
    }
   ],
   "source": [
    "# Save user_comments DataFrame to a newline delimited JSON file\n",
    "user_comments.to_json('user_comments.json', orient='records', lines=True)\n",
    "\n",
    "print(\"user_comments saved to 'user_comments.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "65a2c068-6877-4dd4-8772-97cd8774df16",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_comments.to_csv('user_comments.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
